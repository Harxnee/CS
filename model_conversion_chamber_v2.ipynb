{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b30378-881a-42f9-8a96-d52adf0bd126",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7194d918ad438e977186db8384063a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e2ef6c-9bce-4eb8-98ba-585442a789be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b25984b-2979-42d0-96e7-57c885ac7dba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from optimum.intel import OVModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbeb0b4e-4fc3-44e5-8d42-e866cca0377f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b7b619-f6ea-4d77-83a1-8d335eb8a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "model_id = \"OjasPatil/intel-llama2-7b-test3\"\n",
    "# model_id = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea6f1327-b587-4647-890e-dce69d7be43b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2e37536d3c4f26857f8df216218ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Can't determine type of OV quantization config. Please specify explicitly whether you intend to run weight-only quantization or not with `weight_only` parameter. Creating an instance of OVWeightQuantizationConfig.\n",
      "The model weights will be quantized to int8.\n",
      "Using framework PyTorch: 2.3.0+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/data/venv/openvino_notebooks/openvino_2024.1.0_python3.10/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
      "/data/venv/openvino_notebooks/openvino_2024.1.0_python3.10/.venv/lib/python3.10/site-packages/optimum/exporters/openvino/model_patcher.py:323: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│              8 │ 100% (226 / 226)            │ 100% (226 / 226)                       │\n",
      "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a9922656e64ba3a6a55d5073417b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "model = OVModelForCausalLM.from_pretrained(model_id, \n",
    "                                          # low_cpu_mem_usage=True,\n",
    "                                          # return_dict=True,\n",
    "                                          # torch_dtype=torch.float16,\n",
    "                                          # device_map=device_map,\n",
    "                                          export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a150cd-264d-4e69-9468-f9d8d06b076c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4723aca4-192b-47e2-b863-01a7b7180071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3e841b2-95b2-4b4f-b78e-cbce3a512852",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]Do you give support for free Intel toolkits?[/INST]</s> We do not provide support for free Intel toolkits.  For support for free toolkits, please visit the Intel Developer Zone support center.  For paid toolkits, please contact your account manager or the Intel Software Development Group for support.  For more information, see the free toolkit FAQ. For paid toolkits, please contact your account manager or the\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Who is Pikachu?\"\n",
    "prompt = \"Do you give support for free Intel toolkits?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n",
    "\n",
    "# vary of the format\n",
    "result = pipe(f\"<s>[INST]{prompt}[/INST]</s>\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a6b4321-9513-4440-ab5a-60f996638feb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24048928c34b4e19877d3bf9c3a56f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]Do you offer support for free Intel toolkits?[/INST]</s> Yes, free Intel toolkits are eligible for Priority Support.Ћ For free Intel toolkits, Priority Support is available via email only. Please submit your support requests to Intel® Software Development and Technical Support. For free Intel toolkits, Priority Support is available via email only. Please submit your support requests to Intel® Software Development and Technical Support\n"
     ]
    }
   ],
   "source": [
    "# model_id is being used and not model\n",
    "# model_id -> huggingface model\n",
    "# model -> openvino model\n",
    "# dont use this method, it uses the huggingface model and not the openvino model\n",
    "\n",
    "# prompt = \"Who is Pikachu?\"\n",
    "# # prompt = \"Do you offer support for free Intel toolkits?\"\n",
    "# pipe = pipeline(task=\"text-generation\", model=model_id, tokenizer=tokenizer, max_length=100)\n",
    "\n",
    "# # vary of the format\n",
    "# result = pipe(f\"<s>[INST]{prompt}[/INST]</s>\")\n",
    "# print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2e159c3-d546-419f-85fc-d69a623bddcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"custom_ov_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (OpenVINO Notebooks 2024.1.0)",
   "language": "python",
   "name": "openvino_notebooks_2024.1.0_python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
